{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions to select, read and play the dataset sounds are provided in the ``classification`` directory. <br>\n",
    "\n",
    "As for the H1, you will have to fill some short pieces of code, as well as answer some questions. We already created cells for you to answer the questions to ensure you don't forget it ;). <br>\n",
    "You will find the zones to be briefly filled  with a ``### TO COMPLETE`` in the cells below.\n",
    "\n",
    "<font size=6 color=#009999> 3. Probability vector and memory  [~30min-1h] </font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "model_dir = \"data/models/\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 3.1. Probability vector </font> <br>\n",
    "\n",
    "A clear drawback of the models considered in ``hands_on_classif2_audio_student.ipynb`` is that they only output the most probable class, but do not provide any confidence estimate of this prediction. It is generally better to output a vector of probabilities for all the classes at each prediction, hence allowing the models to hesitate between different classes. \n",
    "Remember a vector of probability can be defined as \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb P \\{ i \\} \\in [0,1], ~~\\sum_i \\mathbb P \\{ i \\} = 1. \n",
    "\\end{equation*}\n",
    "\n",
    "There are many ways to do so:\n",
    "\n",
    "- **Adapt the models**, e.g. for the ``KNN`` classifier, give the probability of class ``i`` as the ratio between the number of neighbours with label ``i`` and the total number of considered neighbours ``K``.\n",
    "- **Use other models**: a ``CNN`` classifier is suited for outputting probability values for each class.\n",
    "- **Compare with old predictions**: the probability of class ``i`` may simply be given as the ratio of its appearances in the (arbitrarily chosen) ``N`` last predictions.\n",
    "\n",
    "The last bullet implies the use of old predictions to compute a probability estimate. This leads to the notion of memory in the predictions, that we discuss in the second part of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by creating a dataset ``myds`` and taking the model trained in ````hands_on_classif2_audio.ipynb````.\n",
    "\n",
    "Don't forget to normalize your feature vectors as well as reduce their dimensionality if you trained your model with such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE - Uncomment the following line\n",
    "# model_knn = pickle.load(open(..., 'rb')) # Write your path to the model here!\n",
    "normalize = True\n",
    "pca = None\n",
    "\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(\n",
    "    dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0, normalize=normalize, pca=pca\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-Source code makes life easy! The ``KNeighborsClassifier`` from Sklearn already contains a ``predict_proba`` method. Start getting some intuition on this probability vector by playing with the chosen feature vector. <br>\n",
    "\n",
    "Run the following code many times by changing  ``cls_index``. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "cls_index = [\"fire\", 0]\n",
    "myds.display(cls_index)\n",
    "thisfv = myds.__getitem__(cls_index).reshape(-1)\n",
    "\n",
    "# this artefact is necessary because the 'predict' function expects a matrix_like input.\n",
    "mat = np.zeros((2, len(thisfv)))\n",
    "mat[0] = thisfv\n",
    "\n",
    "prediction_knn = model_knn.predict(mat)\n",
    "print(\"Class predicted by KNN:\", prediction_knn[0])\n",
    "\n",
    "proba_knn = model_knn.predict_proba(mat)\n",
    "plt.figure()\n",
    "plt.bar(classnames, proba_knn[0])\n",
    "plt.title(\"Probability of each class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "When the classifier miss-predicts, how distributed is the probability vector? Is it good news? How can we exploit that probability distribution for the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=#009999> 3.2. Memory </font> <br>\n",
    "\n",
    "No matter if the predictions are one class only or probability vectors, as it is natural that consecutive feature vectors belong to the same class if the sound type changes slowlier than the duration of a feature vector, it can be helpful to link consecutive predictions and see how similar they are to either strengthen or decrease our confidence in the current guess. <br>\n",
    "\n",
    "Here, we will compare the predictions made on consecutive feature vectors belonging to the same 5s-long sound. \n",
    "Run the following code with different ``class_id``'s and different ``num``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "num = 0  # 0 (default), 1, ..., 39\n",
    "\n",
    "sound = dataset[\"birds\", num]\n",
    "audio = AudioUtil.open(sound)\n",
    "audio = AudioUtil.resample(audio, 11025)\n",
    "AudioUtil.play(audio)\n",
    "\n",
    "\"Bar charts for each window\"\n",
    "n_win = 5\n",
    "probs = np.zeros((n_win, len(classnames)))\n",
    "for window in range(n_win):\n",
    "    sub_aud = (audio[0][window * 11025 :], audio[1])\n",
    "    sub_aud = AudioUtil.pad_trunc(sub_aud, 950)\n",
    "    sgram = AudioUtil.melspectrogram(sub_aud, Nmel=20)\n",
    "    ncol = int(1000 * 11025 / (1e3 * 512))\n",
    "    sgram = sgram[:, :ncol]\n",
    "    fv = sgram.reshape(-1)\n",
    "\n",
    "    ### TO COMPLETE - Eventually normalize and reduce feature vector dimensionality\n",
    "\n",
    "    probs[window, :] = model_knn.predict_proba([fv])[0]\n",
    "\n",
    "\"Mean bar chart\"\n",
    "plt.figure()\n",
    "for window in range(n_win):\n",
    "    plt.bar(\n",
    "        np.arange(len(classnames)) * 2 * n_win + window,\n",
    "        probs[window, :],\n",
    "        alpha=0.9,\n",
    "        label=f\"Window {window}\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.gca().set_xticks(np.arange(len(classnames)) * 2 * n_win + 2)\n",
    "plt.gca().set_xticklabels(classnames)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(classnames)), np.mean(probs, axis=0))\n",
    "plt.gca().set_xticks(np.arange(len(classnames)))\n",
    "plt.gca().set_xticklabels(classnames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Are the bar plots similar between the 5 windows of the same 5s-long sound? With the default sound, how often does the right class win?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is relevant to combine $N$ consecutive feature vectors, there are many ways to output a prediction from it:\n",
    "- **Naive**: select the class that has the highest probability among all the considered feature vectors.\n",
    "- **Majority voting**: simply select the class that appears most often as the maximum probability of a feature vector.\n",
    "- **Average the feature representation**: compute the average of all feature vectors and classify from this average.\n",
    "- **Maximum Likelihood**: take a probabilistic approach and consider selecting class $i$ as\n",
    "    $$\n",
    "        \\text{argmax}_i~ \\log \\bigg(\\prod_{n=0}^{N-1} P(y[n]=i) \\bigg)\n",
    "        = \\text{argmax}_i~ \\sum_{n=0}^{N-1} \\log P(y[n]=i)\n",
    "    $$\n",
    "    with $y[n]$ the model prediction for the feature vector $n$.\n",
    "\n",
    "It will be part of your work to decide how you want to exploit the time information in your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all the necessary material to test a new classification model and make some objective analysis of its performances. <br>\n",
    "Follow the instructions on Moodle [written here](https://moodle.uclouvain.be/mod/assign/view.php?id=204607) to see what is expected in your ``fifth report (R5)``. <br>\n",
    "A lot of other classification models are already implemented by SKlearn, check the [SKlearn API](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Don't hesitate to read some opinions and discussions on the forums or even articles to help you in the model choice. The most motivated of you are even allowed to give a try to more than one additional model, it is time smartly invested for the upcoming weeks of this project! Although we expect only one characterization in the ``R5``.\n",
    "\n",
    "Also, don't hesitate to get information from the Internet to learn how people use to deal with sound classification. We mention for example this idea of transfer learning that could be interesting for you in the second semester: https://www.youtube.com/watch?v=uCGROOUO_wY&t=1s "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "46df200377d403be22c796785365123e6a374b5da08e8292e6b2afda659c5a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
