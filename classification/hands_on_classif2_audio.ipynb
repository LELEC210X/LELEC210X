{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"Machine learning tools\"\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "import pickle\n",
        "\n",
        "from classification.datasets import Dataset\n",
        "from classification.utils.plots import plot_specgram, show_confusion_matrix, plot_decision_boundaries\n",
        "from classification.utils.utils import accuracy\n",
        "from classification.utils.audio_student import AudioUtil, Feature_vector_DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Useful functions to select, read and play the dataset sounds are provided in ``classification/utils/audio_student.py``. <br>\n",
        "\n",
        "As for the H1, you will have to fill some short pieces of code, as well as answer some questions. We already created cells for you to answer the questions to ensure you don't forget it ;). <br>\n",
        "You will find the zones to be briefly filled  with a ``### TO COMPLETE`` in the cells below.\n",
        "\n",
        "<font size=6 color=#009999> 2. Training and Evaluating models on audio signals [~1h30-2h] </font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "dataset = Dataset()\n",
        "classnames = dataset.list_classes()\n",
        "\n",
        "print(\"\\n\".join(classnames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "fm_dir = \"data/feature_matrices/\" # where to save the features matrices\n",
        "model_dir = \"data/models/\" # where to save the models\n",
        "os.makedirs(fm_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In H1, it was not made explicit what we choose as input for the classification model, a.k.a. ``feature vector`` (it was shown in the illustration). <br>\n",
        "The objective is, on the transmitter side, to compute a feature vector containing enough information about the audio signal we want to classify, but not too much in order to limit the data which has to be transmitted wirelessly. This is why in H1 we implemented the ``Hz2Mel`` conversion: a very simple compression of the frequency content. <br>\n",
        "The feature vector we will use here simply consists in taking the first 20 columns of the melspectrogram, corresponding to ~1s, then reshaping it as a vector. This means each feature vector contains ``400`` coefficients, with 20 columns of 20 mels each.  <br>\n",
        "\n",
        "Once the feature vector has been recovered on the receiver side, we can apply any computation on it to guess the right class this sound belongs to. Today, we will simply reuse the simple KNN and LDA classifiers and look at what we already get. \n",
        "\n",
        "<font size=3 color=#FF0000> Important :</font> <br>\n",
        "The analyses that follow are given as food for thoughts. They are not given as step by step improvements of the classifier.\n",
        "\n",
        "<font size=5 color=#009999> 2.1. Creation of the dataset </font> <br>\n",
        "\n",
        "``Feature_vector_DS`` is a class defined in ``classification/utils/audio_student.py``. <br>\n",
        "The functions ``__len__`` and ``__getitem__`` are implemented, meaning you can call :\n",
        "- ``len(myds)`` to get the number of sounds in it.\n",
        "- ``myds[classname,j]`` to get the melspectrogram of the ``j``-th sound from class ``classname``. <br>\n",
        "\n",
        "Two other useful functions are provided:\n",
        "- ``get_audiosignal`` returning the temporal audiosignal at the specified index.\n",
        "- ``display`` playing the sound and showing the associated mel-spectrogram at the specified index.\n",
        "\n",
        "<font size=3 color=#FF0000> Important :</font> <br>\n",
        "Before being able to run the cells below, you will have to reuse your functions from H1 to fill the missing lines in ``audio_student.py`` at ``###TO COMPLETE`` locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "\n",
        "\"Creation of the dataset\"\n",
        "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
        "\n",
        "\"Some attributes...\"\n",
        "myds.nmel\n",
        "myds.duration\n",
        "myds.shift_pct\n",
        "myds.sr\n",
        "myds.data_aug\n",
        "myds.ncol\n",
        "\n",
        "\n",
        "idx = 0\n",
        "myds.display([\"fire\", idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the cell above many times, you should notice it is always the beginning of the sound that is taken for creating the feature vector. ``shift_pct`` meaning *shift percentage* allows to roll the audio signal with a random factor upper bounded by this value. Change ``shift_pct`` to ``0.2`` and observe what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "\"Random split of 70:30 between training and validation\"\n",
        "train_pct = 0.7\n",
        "\n",
        "featveclen = len(myds[\"fire\",0]) # number of items in a feature vector\n",
        "nitems = len(myds) # number of sounds in the dataset\n",
        "naudio = dataset.naudio # number of audio files in each class\n",
        "nclass = dataset.nclass # number of classes\n",
        "nlearn = round(naudio * train_pct) # number of sounds among naudio for training\n",
        "\n",
        "data_aug_factor = 1\n",
        "class_ids_aug = np.repeat(classnames, naudio*data_aug_factor)\n",
        "\n",
        "\"Compute the matrixed dataset, this takes some seconds, but you can then reload it by commenting this loop and decommenting the np.load below\"\n",
        "X = np.zeros((data_aug_factor*nclass*naudio, featveclen))\n",
        "for s in range(data_aug_factor):\n",
        "    for class_idx, classname in enumerate(classnames):\n",
        "        for idx in range(naudio):\n",
        "            featvec = myds[classname, idx]\n",
        "            X[s*nclass*naudio+class_idx*naudio+idx,:] = featvec       \n",
        "np.save(fm_dir+\"feature_matrix_2D.npy\", X)\n",
        "\n",
        "# X = np.load(fm_dir+\"feature_matrix_2D.npy\")\n",
        "\n",
        "\"Labels\"\n",
        "y = class_ids_aug.copy()\n",
        "\n",
        "print('Shape of the feature matrix : {}'.format(X.shape))\n",
        "print('Number of labels : {}'.format(len(y)))\n",
        "\n",
        "print('Remember the convention shown for the toy example, the feature vectors are arranged on the rows.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You might notice that ``feature_matrix_2D.npy`` has been saved in ``data/feature_matrices/`` and can now be loaded instead of recomputing it at every run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.2. First audio classification, metrics and dataset splitting </font> <br>\n",
        "\n",
        "For now we have only prepared the dataset, it remains to feed it to the classifiers. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "K = 6 # Number of neighbours for the KNN\n",
        "model_knn = KNeighborsClassifier(n_neighbors=K, weights='distance', algorithm='auto', metric='minkowski') #We explicitly write the default parameters of this KNN classifier once so that you know they exist and can be changed\n",
        "\n",
        "model_lda = LDA(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001, covariance_estimator=None) #We explicitly write the default parameters of this LDA classifier once so that you know they exist and can be changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for the toy example, we keep the ``accuracy`` and ``confusion matrix`` as performance metrics. <br>\n",
        "\n",
        "Note that here we are not especially interested in a model selection hence we only split the dataset in training and testing parts but we don't split the training set in learning/validation parts. The models are trained on the entire training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "\"Shuffle then split the dataset into training and testing subsets\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # random_state=1\n",
        "print('Shape of the training matrix : {}'.format(X_train.shape))\n",
        "print('Number of training labels : {}'.format(len(y_train)))\n",
        "\n",
        "model_knn.fit(X_train, y_train)\n",
        "model_lda.fit(X_train, y_train)\n",
        "\n",
        "prediction_knn = model_knn.predict(X_test)\n",
        "prediction_lda = model_lda.predict(X_test)\n",
        "accuracy_knn = accuracy (prediction_knn, y_test)\n",
        "accuracy_lda = accuracy (prediction_lda, y_test)\n",
        "\n",
        "print('Accuracy of KNN with fixed train/validation sets : {:.1f}%'.format(100*accuracy_knn))\n",
        "show_confusion_matrix (prediction_knn, y_test, classnames)\n",
        "print('Accuracy of LDA with fixed train/validation sets : {:.1f}%'.format(100*accuracy_lda))\n",
        "show_confusion_matrix (prediction_lda, y_test, classnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions**: \n",
        "- What would be the expected accuracy if the label predictions were picked at random?\n",
        "- What do you observe in this confusion matrix? Run again the cell above, i.e., Reapply the ``train_test_split`` and tell if your observations are robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play with the ``classname`` and the index ``idx`` to pick feature vectors in the dataset ``myds``, listen to the audio associated to the feature vector, and check if you would have been able to predict the right class by your own. Then compare with the prediction given by your classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "idx = 0\n",
        "classname = \"fire\"\n",
        "myds.display([classname, idx])\n",
        "thisfv = myds[classname,idx].reshape(-1)\n",
        "\n",
        "# this artefact is necessary because the 'predict' function expects a matrix_like input.\n",
        "mat = np.zeros((2, len(thisfv)))\n",
        "mat[0] = thisfv\n",
        "\n",
        "prediction_knn = model_knn.predict(mat)\n",
        "prediction_lda = model_lda.predict(mat)\n",
        "\n",
        "print('Class predicted by KNN:', prediction_knn[0])\n",
        "print('Class predicted by LDA:', prediction_lda[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Furthermore, when training a model and comparing different settings, there is a risk that we will end up choosing optimal parameters that only render good result on our specific case of training and validation set, but ``do not generalize well for additional data``. This is called ``overfitting on the validation set``. To alleviate this, we can perform ``cross-validation (CV)``. A basic approach named ``K-fold CV`` involves partitioning the dataset in ``K`` \"folds\" (subsets) and repetitvely do the following procedure:\n",
        "\n",
        "- Train the model using `K-1` folds as the training data.\n",
        "- Test the model using the last fold as the validation data.\n",
        "\n",
        "The overall performance on each fold is then averaged to obtain the final performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "n_splits = 5\n",
        "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
        "\n",
        "accuracy_knn = np.zeros((n_splits,))\n",
        "accuracy_lda = np.zeros((n_splits,))\n",
        "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
        "  (idx_learn, idx_val) = idx\n",
        "  model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "  prediction_knn = model_knn.predict(X_train[idx_val])\n",
        "  accuracy_knn[k] = accuracy(prediction_knn, y_train[idx_val])\n",
        "\n",
        "  model_lda.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "  prediction_lda = model_lda.predict(X_train[idx_val])\n",
        "  accuracy_lda[k] = accuracy(prediction_lda, y_train[idx_val])\n",
        "\n",
        "print('Mean accuracy of KNN with 5-Fold CV: {:.1f}%'.format(100*accuracy_knn.mean()))\n",
        "print('Std deviation in accuracy of KNN with 5-Fold CV: {:.1f}%'.format(100*accuracy_knn.std()))\n",
        "\n",
        "print('Mean accuracy of LDA with 5-Fold CV: {:.1f}%'.format(100*accuracy_lda.mean()))\n",
        "print('Std deviation in accuracy of LDA with 5-Fold CV: {:.1f}%'.format(100*accuracy_lda.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.3. Scale mismatch and countermeasure </font> <br>\n",
        "\n",
        "In real conditions, you will most probably have a different scale between the feature vectors used for training (in simulation) and the ones you feed in your model to make predictions.\n",
        "This scale mismatch between model training and prediction is difficult to prevent because it depends on multiple factors such as the audio source power, its distance to the microphone, the telecommunication distance. <br>\n",
        "\n",
        "Below, we illustrate the link between the volume of the audio and its distance to the origin of the feature space. At different emission distances, the exact same sound would be heard at a different volume and the associated feature vector would be located at another position in the *feature space*. Eventually, this would result in a completely different classification, which is undesirable.\n",
        "\n",
        "<center> <img src=\"figs/norms.png\" alt=\"\" width=\"350\"/> </center>\n",
        "\n",
        "### Questions:\n",
        "\n",
        "- How could you avoid this dependency on the volume of the sound?\n",
        "- What is represented in the hatched centered area? How would you classify feature vectors in this area?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE \n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play with the ``dB_mismatch`` variable here below and observe its effect on the confusion matrix.\n",
        "\n",
        "On which part of the dataset are we computing this confusion matrix?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "dB_mismatch = 0 # Play with this value\n",
        "X_val_scaled = X_train[idx_val]*10**(-dB_mismatch/20)\n",
        "\n",
        "model_knn = KNeighborsClassifier(n_neighbors=10) \n",
        "model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "prediction_knn = model_knn.predict(X_val_scaled)\n",
        "show_confusion_matrix (prediction_knn, y_train[idx_val], classnames)\n",
        "accuracy_knn = accuracy (prediction_knn, y_train[idx_val])\n",
        "print('Accuracy of KNN: {:.1f}%'.format(100*accuracy_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The simplest countermeasure we can think of is to normalise the feature vector (i.e. unitize its norm) prior to use, both for training and testing. Remember how this normalization could be visualized in ``hands_on_classif1_toy_student.ipynb`` <br>\n",
        "Play again with the ``dB_mismatch`` variable here below and observe its effect on the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "dB_mismatch = 0 # Play with this value\n",
        "\n",
        "X_learn_normalised = X_train[idx_learn]/ np.linalg.norm(X_train[idx_learn], axis=1, keepdims=True)\n",
        "model_knn = KNeighborsClassifier(n_neighbors=10, weights='distance') \n",
        "model_knn.fit(X_learn_normalised, y_train[idx_learn])\n",
        "\n",
        "X_val_scaled = X_train[idx_val]*10**(-dB_mismatch/20)\n",
        "X_val_normalised = X_val_scaled/ np.linalg.norm(X_val_scaled, axis=1, keepdims=True)\n",
        "\n",
        "prediction_knn = model_knn.predict(X_val_normalised)\n",
        "show_confusion_matrix (prediction_knn, y_train[idx_val], classnames)\n",
        "accuracy_knn = accuracy (prediction_knn, y_train[idx_val])\n",
        "print('Accuracy of KNN: {:.1f}%'.format(100*accuracy_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**: \n",
        "- What will happen with this normalisation countermeasure when there is no sound around the microphone? Is this desirable? How could you deal with it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.4. Dimensionality reduction </font> <br>\n",
        "\n",
        "It is sometimes good practice to reduce the dimensionality of a signal in order to get the main components of their distribution. A motivation is that usual norms behave counter-inuitively in high dimension. It also further reduces the memory cost of the feature vector. To reduce the dimensionality, we will use the ``Principal component analysis (PCA)`` proposed by sklearn. See the [associated Wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis). Recall: the PCA consists in reducing the dimensionality of data vectors encoded in $\\boldsymbol X \\in \\mathbb R^{d\\times N}$ to only $p \\ll d$ dimensions as\n",
        "\n",
        "$$\n",
        "    \\boldsymbol Y = \\boldsymbol V_p^\\top \\boldsymbol X \\in \\mathbb R^{p\\times N},\n",
        "$$\n",
        "\n",
        "where the SVD of the covariance matrix writes as $\\hat{\\boldsymbol\\Sigma}_{\\boldsymbol X} = \\frac{1}{d} \\boldsymbol{XX}^\\top = \\boldsymbol{U\\Sigma V}^\\top$, and $\\boldsymbol V_p$ is the subselection of the first $p$ columns of $\\boldsymbol V$. \n",
        "\n",
        "For our application, reducing the dimensionality of the data can be helpful for compressing the packet size to be transmitted wirelessly. Indeed, once learned during training, $\\boldsymbol V_p$ can be hardcoded on the transmitter side.\n",
        "\n",
        "Starting with a PCA to 2D for visualization, see how hard it is to separate the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "n=2 # Number of principal components kept\n",
        "pca = PCA(n_components=n,whiten=True)\n",
        "X_learn_reduced = pca.fit_transform(X_train[idx_learn])\n",
        "X_val_reduced = pca.transform(X_train[idx_val])\n",
        "\n",
        "print('Shape of the reduced training matrix : {}'.format(X_learn_reduced.shape))\n",
        "\n",
        "y_train_num = np.zeros(y_train.shape)\n",
        "for i, classname in enumerate(classnames):\n",
        "    y_train_num[y_train==classname] = i\n",
        "\n",
        "K = 10\n",
        "model_knn = KNeighborsClassifier(n_neighbors=K)\n",
        "model_knn.fit(X_learn_reduced, y_train_num[idx_learn])\n",
        "prediction_knn = model_knn.predict(X_val_reduced)\n",
        "accuracy_knn = accuracy(prediction_knn, y_train_num[idx_val])\n",
        "\n",
        "model_lda = LDA()\n",
        "model_lda.fit(X_learn_reduced, y_train_num[idx_learn])\n",
        "prediction_lda = model_lda.predict(X_val_reduced)\n",
        "accuracy_lda = accuracy(prediction_lda, y_train_num[idx_val])\n",
        "\n",
        "fig = plt.figure()\n",
        "axs = [fig.add_axes([0.0, 0.0, 0.4, 0.9]), fig.add_axes([0.6, 0.0, 0.4, 0.9])]\n",
        "plot_decision_boundaries(X_learn_reduced,y_train_num[idx_learn],ax=axs[0],model=model_knn,legend=classnames,title='KNN')\n",
        "plot_decision_boundaries(X_learn_reduced,y_train_num[idx_learn],ax=axs[1],model=model_lda,legend=classnames,title='LDA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**: \n",
        "- From the decision boundaries shown here above, can you explain why the ``handsaw`` class is less often chosen than the other classes for the ``KNN`` classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "n=5 # Number of principal components kept\n",
        "pca = PCA(n_components=n,whiten=True)\n",
        "X_learn_reduced = pca.fit_transform(X_train[idx_learn])\n",
        "X_val_reduced = pca.transform(X_train[idx_val])\n",
        "\n",
        "print('Shape of the reduced learning matrix : {}'.format(X_learn_reduced.shape))\n",
        "\n",
        "K = 10\n",
        "model_knn = KNeighborsClassifier(n_neighbors=K, weights='distance')\n",
        "model_knn.fit(X_learn_reduced, y_train[idx_learn])\n",
        "prediction_knn = model_knn.predict(X_val_reduced)\n",
        "accuracy_knn = accuracy(prediction_knn, y_train[idx_val])\n",
        "\n",
        "model_lda = LDA()\n",
        "model_lda.fit(X_learn_reduced, y_train[idx_learn])\n",
        "prediction_lda = model_lda.predict(X_val_reduced)\n",
        "accuracy_lda = accuracy(prediction_lda, y_train[idx_val])\n",
        "\n",
        "print('Accuracy of the KNN : {:.1f}%'.format(100*accuracy_knn))\n",
        "show_confusion_matrix(prediction_knn, y_train[idx_val], classnames)\n",
        "print('Accuracy of the LDA : {:.1f}%'.format(100*accuracy_lda))\n",
        "show_confusion_matrix(prediction_lda, y_train[idx_val], classnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.5. Analysis of the hyperparameters </font> <br>\n",
        "\n",
        "Finally, we can inspect the influence of ``hyperparameters`` as we did for the toy example. <br>\n",
        "Let us start by analyzing the influence of the number of neighbours $K$ in the KNN. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "Ks = np.arange(6,15, 1)\n",
        "accuracies_knn = np.zeros((len(Ks), n_splits))\n",
        "for i,K in enumerate(Ks):\n",
        "    model_knn = KNeighborsClassifier(n_neighbors=K, weights='distance') \n",
        "    for k, idx in enumerate(kf.split(X_train,y_train)):\n",
        "            (idx_learn, idx_val) = idx\n",
        "            model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "            prediction_knn = model_knn.predict(X_train[idx_val])\n",
        "            accuracies_knn[i,k] = accuracy(prediction_knn, y_train[idx_val])\n",
        "means_knn = accuracies_knn.mean(axis=1)\n",
        "stds_knn = accuracies_knn.std(axis=1)\n",
        "\n",
        "\"Plot\"\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(Ks, means_knn, '.-b', label='KNN')\n",
        "plt.fill_between(Ks,means_knn-stds_knn,means_knn+stds_knn,alpha=0.2,color='b')\n",
        "plt.ylim(0,1)\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we consider both ``K`` and the number of principal components ``n``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "Ks = np.arange(1,10)\n",
        "n_comps = np.arange(2, 15) # number of principal components kept for the PCA\n",
        "accuracies_knn = np.zeros( (len(Ks), len(n_comps)) )\n",
        "accuracies_lda = np.zeros(len(n_comps)) \n",
        "\n",
        "for j, n in enumerate(n_comps):\n",
        "    for idx_learn, idx_val in kf.split(X_train,y_train):\n",
        "        pca = PCA(n_components=n,whiten=True)\n",
        "        X_learn_reduced = pca.fit_transform(X_train[idx_learn])\n",
        "        X_val_reduced = pca.transform(X_train[idx_val])\n",
        "        for i,K in enumerate(Ks):\n",
        "            model_knn = KNeighborsClassifier(n_neighbors=K)\n",
        "            model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "            prediction_knn = model_knn.predict(X_train[idx_val])\n",
        "            accuracies_knn[i,j] += accuracy(prediction_knn, y_train[idx_val])\n",
        "        \n",
        "        model_lda = LDA()\n",
        "        model_lda.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "        prediction_lda = model_lda.predict(X_train[idx_val])\n",
        "        accuracies_lda[j] += accuracy(prediction_lda, y_train[idx_val])\n",
        "\n",
        "accuracies_knn /= n_splits\n",
        "accuracies_lda /= n_splits\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "axs = [fig.add_axes([0.0, 0.0, 0.4, 0.9]), fig.add_axes([0.6, 0.0, 0.4, 0.9])]\n",
        "\n",
        "im0 = axs[0].imshow(100*accuracies_knn, cmap='jet', origin='lower')\n",
        "cbar = fig.colorbar(im0, ax=axs[0])\n",
        "cbar.set_label('Accuracy (%)')\n",
        "axs[0].set_xlabel('n_PCA')\n",
        "axs[0].set_ylabel('K')\n",
        "axs[0].set_xticks(list(np.arange(len(n_comps))))\n",
        "axs[0].set_xticklabels(list(n_comps))\n",
        "axs[0].set_yticks(list(np.arange(len(Ks))))\n",
        "axs[0].set_yticklabels(list(Ks))\n",
        "axs[0].set_title('KNN')\n",
        "\n",
        "axs[1].plot(accuracies_lda*100)\n",
        "axs[1].set_xlabel('n_PCA')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].set_title('LDA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**: \n",
        "- Do you observe some dependency of the accuracy on these parameters? If so, which one(s)? If not, discuss what it tells about the considered model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the question above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.6. Augmenting the data </font> <br>\n",
        "\n",
        "In order to make our classifier more robust to some common transformations of the audio signal such as ``time shift``, ``AWGN``, or a ``transfer function``, an idea consists in feeding the classifier with such transformations. A popular approach is to create new feature vectors based on transformed versions of the sounds from the original dataset, this is called ``data augmentation``. Data augmentation is also often used when there is few data to train a model. <br>\n",
        "\n",
        "The functions to augment your data are written in ``utils/audio_student.py``, we already implemented ``time_shift``, ``echo`` and ``spectro_aug_timefreq_masking`` for you. Try to implement ``scaling``, ``add_noise``, ``filter``, ``add_bg`` and even more data augmentation techniques if you want, and check their working in the cell below. <br>\n",
        "\n",
        "<u>Tip</u>: to avoid restarting the notebook kernel for each modification, you can temporarily insert the ``AudioUtil`` class in a new cell and make your tests until it is working as expected. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "myds.data_aug = None # Ensure\n",
        "\n",
        "cls_index = [\"birds\", 4]\n",
        "\n",
        "sound = dataset.__getitem__(cls_index)\n",
        "name = dataset.__getname__(cls_index)\n",
        "audio = AudioUtil.open(sound)\n",
        "\n",
        "AudioUtil.play(audio)\n",
        "audio2 = AudioUtil.resample(audio, 11025)\n",
        "audio2 = AudioUtil.pad_trunc(audio2, 5000)\n",
        "\n",
        "audio3 = AudioUtil.time_shift(audio2, 0.4)\n",
        "audio4 = AudioUtil.scaling(audio2) \n",
        "audio5 = AudioUtil.add_noise(audio2, sigma=1e-2)\n",
        "audio6 = AudioUtil.echo(audio2, 3)\n",
        "audio7 = AudioUtil.add_bg(audio2, dataset)\n",
        "\n",
        "melspec = AudioUtil.melspectrogram(audio2, fs2=11025)\n",
        "melspec2 = AudioUtil.spectro_aug_timefreq_masking(melspec, max_mask_pct=0.1)\n",
        "\n",
        "\"Plot\"\n",
        "fig = plt.figure(figsize=(15,4))\n",
        "ax1 = fig.add_axes([0.05, 0.05, 0.28, 0.9])\n",
        "ax2 = fig.add_axes([0.38, 0.05, 0.28, 0.9])\n",
        "ax3 = fig.add_axes([0.7, 0.05, 0.28, 0.9])\n",
        "\n",
        "ax1.plot(audio2[0], label='Original')\n",
        "ax1.plot(audio3[0]+1, label='Time shifted')\n",
        "ax1.plot(audio4[0]+2, label='Rescaled')\n",
        "ax1.plot(audio5[0]+3, label='Noisy')\n",
        "ax1.plot(audio6[0]+4, label='With echos')\n",
        "ax1.plot(audio7[0]+5, label='With background sound')\n",
        "ax1.legend()\n",
        "\n",
        "plot_specgram(melspec, ax2, is_mel=True, title=name, tf = len(audio2[0])/audio2[1])\n",
        "ax2.set_title('Melspectrogram')\n",
        "plot_specgram(melspec2, ax3, is_mel=True, title=name, tf = len(audio2[0])/audio2[1])\n",
        "ax3.set_title('Corrupted melspectrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create a new augmented dataset and observe if the classification results improve. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "myds.mod_data_aug(['add_bg'])\n",
        "y_aug = np.repeat(classnames, dataset.naudio*myds.data_aug_factor) # Labels\n",
        "\n",
        "\"Compute the matrixed dataset, this takes some seconds, but you can then reload it by commenting this loop and decommenting the np.load below\"\n",
        "X_aug = np.zeros((myds.data_aug_factor*nclass*naudio, featveclen))\n",
        "for s in range(myds.data_aug_factor):\n",
        "    for idx in range(dataset.naudio):\n",
        "        for class_idx, classname in enumerate(classnames):\n",
        "            featvec = myds[classname, idx]  \n",
        "            X_aug[s*nclass*naudio+class_idx*naudio+idx,:] = featvec \n",
        "np.save(fm_dir+\"feature_matrix_2D_aug.npy\", X_aug)\n",
        "\n",
        "# X_aug = np.load(fm_dir+\"feature_matrix_2D_aug.npy\")\n",
        "\n",
        "print('Shape of the feature matrix : {}'.format(X_aug.shape))\n",
        "print('Number of labels : {}'.format(len(y_aug)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO RUN\n",
        "K = 10 # Number of neighbours\n",
        "model = KNeighborsClassifier(n_neighbors=K) \n",
        "\n",
        "accuracy_aug = np.zeros((n_splits,))\n",
        "for k, idx in enumerate(kf.split(X_aug,y_aug)):\n",
        "  (idx_train, idx_test) = idx\n",
        "  model.fit(X_aug[idx_train], y_aug[idx_train])\n",
        "  prediction_aug = model.predict(X_aug[idx_test])\n",
        "  accuracy_aug[k] = accuracy(prediction_aug, y_aug[idx_test])\n",
        "\n",
        "print('Mean accuracy with 5-Fold CV: {:.1f}%'.format(100*accuracy_aug.mean()))\n",
        "print('Std deviation in accuracy with 5-Fold CV: {:.1f}%'.format(100*accuracy_aug.std()))\n",
        "show_confusion_matrix(prediction_aug, y_aug[idx_test], classnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions**:\n",
        "- Can you see an improvement of the classification result compared to the non augmented dataset? Try to interpret your answer by thinking about the distribution of points in a data space (as with the toy example), what does it imply to augment the data in terms of distribution of points in the data space?\n",
        "- With the ``add_bg`` augmentation technique, where are the additive background signals coming from? It is a good thing?\n",
        "- What transformations are most likely to be realistic in your application? What is the most efficient way to integrate these alterations in your classification task? ``Hint``: it does not require augmenting your data in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the question above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.7. Getting it all together </font> <br>\n",
        "\n",
        "Now that some aspects to be considered during the model training and analysis have been presented, it remains to train and save a final model that will be used for further predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "\n",
        "# [1] Create dataset of feature vectors and split it.\n",
        "# (optional) with data augmentation\n",
        "myds = ...\n",
        "# X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "\n",
        "# [2] (optional) Data normalization\n",
        "\n",
        "# [3] (optional) dimensionality reduction.\n",
        "...\n",
        "\n",
        "# [4] Model training and selection.\n",
        "model = ...\n",
        "...\n",
        "\n",
        "# [5] Save the trained model, eventually the pca.\n",
        "filename = 'model.pickle'\n",
        "pickle.dump(model, open(model_dir+filename, 'wb'))\n",
        "\n",
        "# [6] Evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 2.8. Debriefing </font> <br>\n",
        "**Questions** : \n",
        "\n",
        "1) from what we have done in this notebook, can you already identify some weaknesses in the feature vector computation and classification pipeline? You can make a list here below, and eventually write some short ideas for improvement. This will help you later :)\n",
        "2) Do you remember what is the time duration of a feature vector? What happens if no sound is produced during the acquisition time of a feature vector?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the question above"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "46df200377d403be22c796785365123e6a374b5da08e8292e6b2afda659c5a28"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
