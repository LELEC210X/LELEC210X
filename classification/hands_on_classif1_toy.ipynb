{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Classification \n",
    "\n",
    "For this hands-on session, let us go back to the signal processing aspects.\n",
    "Let us show again the block-diagram of the project. The very first hands-on session was focused on computing the feature vector from a sound vector. \n",
    "Here, we are going to insert a simple classification model in our chain. This is the core of our application. <br>\n",
    "\n",
    "<center> <img src=\"figs/block-diagram.png\" alt=\"\"  width=\"650\" height=\"350\"/> </center>\n",
    "\n",
    "There exists a lot of different classification models in the literature. In this notebook we propose to play with two simple and intuitive classifiers: the K-Nearest-Neighbour (``KNN``) classifier and the Linear Discriminant Analysis (``LDA``) classifier. The related Wikipedia page are [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and [LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis). <br>\n",
    "Made simple, KNN and LDA are ``supervised machine learning algorithms``. For KNN, each new point to be classified is compared with its K nearest neighbours among labelled data (i.e. points whose class is known) in the dataspace with respect to a chosen distance metric. For LDA, the data distributions of the classes are assumed to be normal, i.i.d. and with the same covariance. The decision boundaries are set to areas in the distribution space where the log-likelihood that the data belongs to another class is higher than the true class. It is worth noticing that the LDA classifier is a parametric model while KNN is not. Indeed, KNN has only one hyperparameter $K$, but the learning phase simply consist of saving the learning set in memory to be used later for inference.  <br>\n",
    "\n",
    "To avoid reinventing the wheel, we will use the implementations provided by ``sklearn``. You are strongly encouraged to have a look at the documentation [for the KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [for the LDA](https://scikit-learn.org/stable/modules/lda_qda.html?highlight=lda). We will also use ``pickle`` for saving and loading the trained models. <br>\n",
    "<!-- In case of an error with the utils folder (folder not found), you may need to launch Jupyter with the directory where the code to execute is located. To do so, open the Anaconda Prompt (if you are using Anaconda) and type ``jupyter notebook --notebook-dir=$YOUR PATH$``. <br> -->\n",
    "Useful functions to select, read and play the dataset sounds are provided in ``src/classification``. <br>\n",
    "\n",
    "As for the first hands-on session of the semester ``hands_on_feature_vectors.ipynb`` (written ``H1`` below for compactness), you will have to fill some short pieces of code, as well as answer some questions. We already created cells for you to answer the questions to ensure you don't forget it ;). <br>\n",
    "You will find the zones to be briefly filled  with a ``### TO COMPLETE`` in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from classification.datasets import Dataset, get_cls_from_path\n",
    "from classification.utils.plots import plot_decision_boundaries, show_confusion_matrix\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=#009999> 1. Getting intuition with a toy example [~1h-1h30] </font> <br>\n",
    "\n",
    "Before going deep into classifying the complicated feature vectors we designed in H1, let us analyze a toy example. <br>\n",
    "It is always good practice to make some trials on a simplified version of a problem before tackling it. It prevents from a big amount of time loss and some strange dark bugs. <br>\n",
    "\n",
    "The convention used by ``sklearn`` is that $\\boldsymbol{X}$ is a matrix whose height is the number $N$ of samples in the dataset and width is the dimension $p$ of each feature vector, as depicted here below (one colour is one class). It writes \n",
    "\\begin{equation*}\n",
    "    \\boldsymbol X = (\\boldsymbol x_1, \\cdots, \\boldsymbol x_N)^\\top\n",
    "\\end{equation*}\n",
    "\n",
    "with $\\boldsymbol x_i \\in \\mathbb R^p$ the feature vector of the $i$-th sample whose class is encoded in $y_i$, i.e. \n",
    "\\begin{equation*}\n",
    "    y_i \\in \\{\\text{'Class 0'}, \\cdots, \\text{'Class 3'}\\}\n",
    "\\end{equation*}\n",
    "\n",
    "<center> <img src=\"figs/Xy.svg\" alt=\"\"  width=\"500\"/> </center>\n",
    "\n",
    "<font size=5 color=#009999> 1.0. Generating data for the toy example </font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "plot3D = False\n",
    "normalize = False\n",
    "N = 100  # Number of points per subcluster. (defaut: 100)\n",
    "n_classes = 4  # Number of differents classes. (default: 4)\n",
    "n_subclusters = 1  # Number of subclusters of points per class. (defaut: 1)\n",
    "dim = 2  # Dimensionality of the point clouds. (defaut: 2)\n",
    "dim_clusters = 2  # Dimensionality of the clusters arrangment. (default: 2)\n",
    "sig = 0.25  # Noise std. (default: 0.25)\n",
    "np.random.seed(\n",
    "    1\n",
    ")  # For some reproducibility in the results, you can change the seed or remove this line if desired. (default: 1)\n",
    "\n",
    "M = N * n_subclusters  # Number of points per class\n",
    "\n",
    "\"Generate the data\"\n",
    "cluster_centers = np.concatenate(\n",
    "    (\n",
    "        np.random.randn(n_classes * n_subclusters, dim_clusters),\n",
    "        np.zeros((n_classes * n_subclusters, dim - dim_clusters)),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "centers = np.repeat(cluster_centers, N, axis=0)\n",
    "noise = sig * np.random.randn(n_classes * M, dim)\n",
    "X = centers + noise\n",
    "if normalize:\n",
    "    X /= np.linalg.norm(X, axis=1, keepdims=True)\n",
    "y = np.repeat(np.arange(n_classes), M)\n",
    "\n",
    "print(\n",
    "    \"Beware, the points are plotted in 2D but can belong to a space with more dimensions!\"\n",
    ")\n",
    "\n",
    "\"Plot\"\n",
    "cm = \"brg\"\n",
    "edgc = \"k\"\n",
    "fig = plt.figure()\n",
    "if plot3D:\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=cm, edgecolor=edgc)\n",
    "else:\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    scatterd = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm, edgecolors=edgc)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "handles, labels = scatterd.legend_elements(prop=\"colors\")\n",
    "ax.legend(handles, labels, title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 1.1. Metrics and model evaluation </font> <br>\n",
    "In order to objectively evaluate the performance of your classification model, the use of metrics is necessary.\n",
    "[See some examples here.](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226) <br>\n",
    "Throughout this notebook and for this project, the two metrics we will use are:\n",
    "* Accuracy = $\\frac{\\text{\\# Good predictions}}{\\text{Total \\# predictions}} = \\frac{\\text{TP+TN}}{\\text{TP+FP+FN+TN}}$.\n",
    "* Confusion matrix: add $1$ to the counter at position $(i,j)$ if the model predicted $i$ but the true label was $j$.\n",
    "\n",
    "If you are interested, there exists a lot of other metrics. Checkout on the internet. <br>\n",
    "\n",
    "We provide the code for the KNN classifier. For you to start handling sklearn implementations, we let you ``declare a LDA classifier and compute its accuracy on the test set``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Shuffle the data then split in training and testing sets\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y\n",
    ")  # 'stratify=y' ensures we pick the same number of samples per class during the splitting\n",
    "\n",
    "K = 10  # Number of neighbours\n",
    "model_knn = KNeighborsClassifier(n_neighbors=K)  # Declare the KNN classifier\n",
    "model_knn.fit(X_train, y_train)  # Train the classifier\n",
    "prediction_knn = model_knn.predict(X_test)\n",
    "accuracy_knn = np.sum(prediction_knn - y_test == 0) / X_test.shape[0]\n",
    "print(f\"Accuracy KNN : {100 * accuracy_knn:.2f} %\")\n",
    "\n",
    "### TO COMPLETE\n",
    "model_lda = ...  # Declare the LDA classifier with parameters by default.\n",
    "...  # Train the classifier\n",
    "prediction_lda = ...  # Predict the classes of the testing set\n",
    "accuracy_lda = np.sum(prediction_lda - y_test == 0) / X_test.shape[0]\n",
    "print(f\"Accuracy LDA : {100 * accuracy_lda:.2f} %\")\n",
    "\n",
    "\"Plot\"\n",
    "if dim == 2:\n",
    "    # Plot the decision boundary.\n",
    "    s = 15.0\n",
    "    fig = plt.figure()\n",
    "    axs = [fig.add_axes([0.0, 0.0, 0.4, 0.9]), fig.add_axes([0.6, 0.0, 0.4, 0.9])]\n",
    "    plot_decision_boundaries(\n",
    "        X, y, model_knn, ax=axs[0], legend=labels, title=\"KNN\", s=s\n",
    "    )\n",
    "    plot_decision_boundaries(\n",
    "        X, y, model_lda, ax=axs[1], legend=labels, title=\"LDA\", s=s\n",
    "    )\n",
    "    # plt.show()\n",
    "\n",
    "axs[0].set_title(\"KNN\")\n",
    "show_confusion_matrix(prediction_knn, y_test, np.arange(n_classes))\n",
    "axs[1].set_title(\"LDA using rbf kernel\")\n",
    "show_confusion_matrix(prediction_lda, y_test, np.arange(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's already time for some analysis (**put the default parameters when unspecified**):\n",
    "\n",
    "1) Which accuracy would you expect from a classifier choosing uniformly at random?\n",
    "2) Can you explain why the classifiers are more confused between classes 0 and 3?\n",
    "3) Play with ``n_classes``, how do the accuracy and the confusion matrix evolve with it?\n",
    "4) Play with ``sig``, how do the accuracy and the confusion matrix evolve with it?\n",
    "5) Fix ``dim=20``, do you observe any change? An intuitive explanation?\n",
    "6) Fix ``dim=20`` and ``dim_clusters=20``, what do you observe?\n",
    "7) Change ``K`` to 200, what is the impact on the confusion matrix? Now choose ``n_subclusters``=2, what happens?\n",
    "8) How different are the decision boundaries between the KNN and the LDA with linear kernels? Which one would work best on the 3-ringes data example provided at the end of this notebook(come back to this question later)?\n",
    "9) Put ``normalize=True``. What do you observe in the data distribution? Think of the data points as if it corresponded to the acquired sounds. In which situation is it interesting to normalize? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 1.2. Dataset splitting and Model choice </font> <br>\n",
    "\n",
    "The usual convention to objectively analyse the performances of learned models is to split the dataset into three sets: ``learning, validation, testing`` where the validation set allows to choose the hyperparameters of each model. \n",
    "\n",
    "<center> <img src=\"figs/dataset_splitting.svg\" alt=\"\"  width=\"500\" height=\"250\"/> </center>\n",
    "\n",
    "All the data in the learning and validation sets is used to train models and choose the hyperparameters that are optimal with respect to the chosen metrics, we call the ensemble the ``training set``. \n",
    "When training a model and comparing different settings, there is a risk that we will end up choosing optimal parameters that only renders good result on our specific case of training and validation set, but ``do not generalize well for additional data``. This is called ``overfitting on the validation set``. To alleviate this, we can perform ``cross-validation (CV)``. A basic approach named ``K-fold CV`` involves partitioning the dataset in ``K`` \"folds\" (subsets) and repetitvely do the following procedure:\n",
    "\n",
    "- Train the model using `K-1` folds as the training data.\n",
    "- Test the model using the last fold as the validation data.\n",
    "\n",
    "The overall performance on each fold is then averaged to obtain the final performance metrics.\n",
    "Alternatives to K-fold CV like [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) or other techniques exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "accuracy_knn = np.zeros((n_splits,))\n",
    "accuracy_lda = np.zeros((n_splits,))\n",
    "for k, idx in enumerate(kf.split(X_train, y_train)):\n",
    "    (idx_learn, idx_val) = idx\n",
    "    model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "    prediction_knn = model_knn.predict(X_train[idx_val])\n",
    "    accuracy_knn[k] = accuracy(prediction_knn, y_train[idx_val])\n",
    "\n",
    "    model_lda.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "    prediction_lda = model_lda.predict(X_train[idx_val])\n",
    "    accuracy_lda[k] = accuracy(prediction_lda, y_train[idx_val])\n",
    "\n",
    "print(\n",
    "    \"Mean accuracy of KNN with {}-Fold CV: {:.1f}%\".format(\n",
    "        n_splits, 100 * accuracy_knn.mean()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Std deviation in accuracy of KNN with 5-Fold CV: {:.1f}% \\n\".format(\n",
    "        100 * accuracy_knn.std()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Mean accuracy of LDA with {}-Fold CV: {:.1f}%\".format(\n",
    "        n_splits, 100 * accuracy_lda.mean()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Std deviation in accuracy of LDA with 5-Fold CV: {:.1f}%\".format(\n",
    "        100 * accuracy_lda.std()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upper analysis, we fixed ``K`` for the KNN. This is called an ``hyperparameter`` of the classification model. Let us now have a look at the effect of this hyperparameter!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "Ks = np.arange(1, 200, 2)\n",
    "accuracies_knn = np.zeros((len(Ks), n_splits))\n",
    "for i, K in enumerate(Ks):\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    for k, idx in enumerate(kf.split(X_train, y_train)):\n",
    "        (idx_learn, idx_val) = idx\n",
    "        model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "        prediction_knn = model_knn.predict(X_train[idx_val])\n",
    "        accuracies_knn[i, k] = accuracy(prediction_knn, y_train[idx_val])\n",
    "means_knn = accuracies_knn.mean(axis=1)\n",
    "stds_knn = accuracies_knn.std(axis=1)\n",
    "\n",
    "\"Plot\"\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.plot(Ks, means_knn, \".-b\", label=\"KNN\")\n",
    "plt.fill_between(Ks, means_knn - stds_knn, means_knn + stds_knn, alpha=0.2, color=\"b\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "- What do you conclude regarding the dependency of the accuracy on K for the KNN in this case? Does it hold for other sets of parameters for the data generation? Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we analysed the performance dependence on the hyperparameters, we can compare the two selected models on the ``test`` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "best_K = Ks[np.argmax(means_knn)]\n",
    "\n",
    "print(f\"Best K for KNN: {best_K}\")\n",
    "\n",
    "model_best_knn = KNeighborsClassifier(n_neighbors=best_K)\n",
    "model_best_knn.fit(X_train, y_train)\n",
    "prediction_best_knn = model_best_knn.predict(X_test)\n",
    "accuracy_best_knn = accuracy(prediction_best_knn, y_test)\n",
    "print(f\"Accuracy best KNN : {100 * accuracy_best_knn:.2f} %\")\n",
    "\n",
    "model_lda = LDA()\n",
    "model_lda.fit(X_train, y_train)\n",
    "prediction_lda = model_lda.predict(X_test)\n",
    "accuracy_lda = accuracy(prediction_lda, y_test)\n",
    "print(f\"Accuracy LDA : {100 * accuracy_lda:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- From the output here above, which model will you choose at the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> Comment </font> <br>\n",
    "\n",
    "``You don't have to understand the code.`` <br>\n",
    "\n",
    "It can happen that the data to be classified is exploitable, but non linearly separable in their ambient space. It can be smart to find a transformation function $\\Phi (\\boldsymbol X)$\n",
    "that would ease the discrimination between your data points. For example, this is the trick used in [SVM](https://scikit-learn.org/stable/modules/svm.html) when radial basis functions or polynomial kernels are used (not shown here). Run the code here below and observe how well we are able to transform three intricate rings of data points into three localised point clouds using a technique called *spectral clustering*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "N = 300  # Number of points per subcluster. (defaut: 100)\n",
    "n_classes = 3\n",
    "M = n_classes * N\n",
    "thetas = np.random.uniform(0, 2 * np.pi, M)\n",
    "\n",
    "X2 = (\n",
    "    (np.array([np.cos(thetas), np.sin(thetas)]))\n",
    "    * (np.arange(M) // N + 0.1 * np.random.randn(M))\n",
    ").T  # The data points\n",
    "\n",
    "sigma = 0.1\n",
    "W = np.zeros((M, M))\n",
    "for i in range(M):\n",
    "    for j in range(i):\n",
    "        dist = (\n",
    "            np.linalg.norm(X2[i, :] - X2[j, :]) ** 2\n",
    "        )  # Should decrease with the distance!\n",
    "        W[i, j] = np.exp(-dist / (2 * sigma**2))\n",
    "        W[j, i] = np.exp(-dist / (2 * sigma**2))\n",
    "\n",
    "D = np.diag(np.sum(W, axis=0))  # Degree matrix for a weigthed graph\n",
    "L = D - W  # Graph Laplacian\n",
    "(U, s, _) = np.linalg.svd(L, full_matrices=True)  # SVD decomposition of L\n",
    "\n",
    "# Keep only K first eigenvectors\n",
    "K = 3\n",
    "# Careful: SVD returns the singular values in DESCENDING order; we thus want to extract the LAST two columns\n",
    "U = U[:, -K:]\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "axs = [fig.add_axes([0.55 * i, 0, 0.45, 1]) for i in range(2)]\n",
    "for i in range(n_classes):\n",
    "    axs[0].scatter(X2[i * N : (i + 1) * N, 0], X2[i * N : (i + 1) * N, 1], label=i)\n",
    "    axs[1].scatter(U[i * N : (i + 1) * N, 0], U[i * N : (i + 1) * N, 1], s=5)\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Data in initial space\")\n",
    "axs[1].set_title(\"Data in transformed space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 1.3. Dimensionality reduction </font> <br>\n",
    "\n",
    "It is sometimes good practice to reduce the dimensionality of a signal in order to get the main components of their distribution. A motivation is that usual norms behave counter-inuitively in high dimension. To reduce the dimensionality, we will use the ``Principal compenent analysis (PCA)`` proposed by sklearn. See the [associated Wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis). We start by illustrating the interest of PCA with a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "N = 100  # Number of points per subcluster. (defaut: 100)\n",
    "n_classes = 4  # Number of differents classes. (default: 4)\n",
    "sig = 0.25  # Noise std. (default: 0.25)\n",
    "np.random.seed(\n",
    "    8\n",
    ")  # For some reproducibility in the results, you can change the seed or remove this line if desired. (default: 1)\n",
    "\n",
    "\"Generate the data\"\n",
    "xc = np.random.randn(n_classes)\n",
    "yc = 0.5 * xc - 0.2\n",
    "cluster_centers = np.concatenate((xc[:, np.newaxis], yc[:, np.newaxis]), axis=1)\n",
    "centers = np.repeat(cluster_centers, N, axis=0)\n",
    "noise = sig * np.random.randn(n_classes * N, 2)\n",
    "X = centers + noise\n",
    "\n",
    "\"Apply PCA on data to reduce dimensionality to 1D\"\n",
    "n = 1  # Number of principal components kept\n",
    "pca = PCA(n_components=n, whiten=True)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "\"Plot\"\n",
    "s = 3.0\n",
    "fig = plt.figure()\n",
    "axs = [fig.add_axes([0.0, 0.0, 0.4, 1.0]), fig.add_axes([0.6, 0.0, 0.4, 1.0])]\n",
    "axs[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "axs[1].set_aspect(\"equal\", adjustable=\"box\")\n",
    "for i in range(n_classes):\n",
    "    axs[0].scatter(X[i * N : (i + 1) * N, 0], X[i * N : (i + 1) * N, 1], label=i, s=s)\n",
    "    axs[1].scatter(X_reduced[i * N : (i + 1) * N], np.zeros(N), label=i, s=s)\n",
    "axs[0].set_title(\"Original data living in 2D\")\n",
    "axs[1].set_title(\"Data projected on the first principal component\")\n",
    "axs[0].set_xlabel(\"$x_1$\")\n",
    "axs[0].set_ylabel(\"$x_2$\")\n",
    "axs[1].set_xlabel(\"$u_1$\")\n",
    "axs[1].set_yticks([])\n",
    "axs[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hands-on session focuses on the simple KNN and LDA classifiers. However, there are many other that are worth giving a try using SKlearn. To give you motivation, run ``plot_classifier_comparison.py``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lelec210x-I3MJv9Ae-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
