{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from classification.datasets import Dataset_augmented\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_trimming.py must be run before executing this cell!\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset_augmented()\n",
    "classnames = dataset.list_classes() #['chainsaw', 'fire', 'fireworks', 'gun']\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "number_audio_files = 280\n",
    "# splitting it into training and test sets\n",
    "for i in range(4):\n",
    "    random_indexes = [np.random.choice(number_audio_files, int(number_audio_files*0.7), replace=False) for _ in range(4)]\n",
    "    test_indexes = [np.setdiff1d(np.arange(number_audio_files), random_indexes[i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"src/classification/datasets/melspectrograms/\"\n",
    "def fv_generator(Nft_num, nmel_num):\n",
    "    Nft = Nft_num\n",
    "    nmel = nmel_num\n",
    "    # threshold = 0.05 # threshold under which we discard the input\n",
    "    \n",
    "    # Computing the mel spectrogram of each audio file and saving in a folder\n",
    "    n_win_files = np.zeros(4*number_audio_files)\n",
    "    \n",
    "    for class_index in range (len(classnames)):\n",
    "        for audio_index in range(number_audio_files):\n",
    "            current_sound = dataset[classnames[class_index], audio_index]\n",
    "            current_audio = AudioUtil.open(current_sound)\n",
    "            current_audio = AudioUtil.resample(current_audio, 11025)\n",
    "            \n",
    "            # we will split the audio into 1 second window, and compute the mel spectrogram of each clip\n",
    "            n_win = (len(current_audio[0]) // 11025) + 1\n",
    "            n_win_files[class_index * number_audio_files + audio_index] = int(n_win)\n",
    "            for window in range(n_win):\n",
    "                sub_aud = (current_audio[0][window * 11025 :], current_audio[1])\n",
    "                sub_aud = AudioUtil.pad_trunc(sub_aud, 950)\n",
    "                sgram = AudioUtil.melspectrogram(sub_aud, Nmel=nmel, Nft=Nft)\n",
    "                ncol = int(11025 / Nft)\n",
    "                sgram = sgram[:, :ncol]\n",
    "                fv = sgram.reshape(-1)\n",
    "                # saving the mel spectrogram in .npy format\n",
    "                np.save(folder_path + classnames[class_index] + str(audio_index) + \"_\" + str(window) + \".npy\", fv)\n",
    "                \n",
    "    fv_len = len(fv)\n",
    "    return fv_len, n_win_files\n",
    "    \n",
    "    \n",
    "def pca_generator(training_indexes, pca_comp):\n",
    "    # Creating all the feature vectors to compute the PCA\n",
    "    total_number_window_training = np.sum(n_win_files[training_indexes[0]]) + np.sum(n_win_files[training_indexes[1]]) + np.sum(n_win_files[training_indexes[2]]) + np.sum(n_win_files[training_indexes[3]])\n",
    "    X_train = np.zeros((int(total_number_window_training), int(fv_len)))\n",
    "    y_train = np.zeros(int(total_number_window_training))\n",
    "    \n",
    "    # we will use the indexes to load the mel spectrograms and compute the PCA\n",
    "    index = 0\n",
    "    for class_index in range (len(classnames)):\n",
    "        for audio_index in training_indexes[class_index]:\n",
    "            for window in range(int(n_win_files[class_index * number_audio_files + audio_index])):\n",
    "                X_train[index, :] = np.load(folder_path + classnames[class_index] + str(audio_index) + \"_\" + str(window) + \".npy\")\n",
    "                y_train[index] = class_index\n",
    "                index += 1\n",
    "                \n",
    "    # normalisation before PCA since the amplitude is not a valuable information\n",
    "    X_train_pca = X_train / np.linalg.norm(X_train, axis=0)\n",
    "    \n",
    "    # PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=pca_comp)\n",
    "    pca.fit(X_train_pca)\n",
    "    \n",
    "    return pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_fv(class_index, audio_indexes):\n",
    "    \"\"\"\n",
    "    This function takes a class index and a list of audio indexes and returns a matrix of size (len(audio_indexes), n_win_files, fv_len)\n",
    "    containing the mel spectrogram of each audio file. It also returns a list of labels (which is the same for all the windows of the same audio file)\n",
    "    \"\"\"\n",
    "    X = np.array([np.zeros((int(n_win_files[idx + class_index * number_audio_files]), fv_len)) for idx in audio_indexes], dtype=object)\n",
    "    label = np.array([[classnames[class_index]] * int(n_win_files[idx + class_index * number_audio_files]) for idx in audio_indexes],dtype=object)\n",
    "    for i, audio_index in enumerate(audio_indexes):\n",
    "        for window in range(int(n_win_files[audio_index + class_index * number_audio_files])):\n",
    "            X[i][window, :] = np.load(folder_path + classnames[class_index] + str(audio_index) + \"_\" + str(window) + \".npy\")\n",
    "            label[i][window] = classnames[class_index]\n",
    "    return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def manual_kfold_random(index_list, n_splits, seed=None):\n",
    "    \"\"\"\n",
    "    Génère des indices pour un k-fold manuel avec répartition aléatoire.\n",
    "\n",
    "    :param index_list: Liste ou array NumPy des indices totaux\n",
    "    :param n_splits: Nombre de partitions (doit être inférieur à la taille de la liste)\n",
    "    :param seed: Optionnel, permet de fixer la graine pour la reproductibilité\n",
    "    :return: Liste de tuples (learning_set, validation_set)\n",
    "    \"\"\"\n",
    "    assert n_splits < len(index_list), \"n_splits doit être inférieur au nombre total d'indices.\"\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)  # Fixe la graine pour des résultats reproductibles\n",
    "\n",
    "    index_list = list(index_list)  # Conversion en liste au cas où c'est un array NumPy\n",
    "    random.shuffle(index_list)  # Mélange les index aléatoirement\n",
    "\n",
    "    fold_size = len(index_list) // n_splits\n",
    "    folds = [index_list[i * fold_size:(i + 1) * fold_size] for i in range(n_splits)]\n",
    "    \n",
    "    if len(index_list) % n_splits != 0:  # Gestion des restes\n",
    "        folds[-1] = list(folds[-1]) + index_list[n_splits * fold_size:]  # Conversion et ajout correct\n",
    "\n",
    "    split_indices = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        validation_set = folds[i]\n",
    "        learning_set = [idx for j, fold in enumerate(folds) if j != i for idx in fold]\n",
    "        split_indices.append((learning_set, validation_set))\n",
    "    \n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def predict_all_probabilities(clf, X):\n",
    "    \"\"\"\n",
    "    Prédit les probabilités pour tous les feature vectors dans X.\n",
    "\n",
    "    :param clf: Classifieur entraîné avec une méthode `.predict_proba()`\n",
    "    :param X: Array de shape (len(classnames), variable, n_win, fv_len)\n",
    "    :return: Liste des probabilités prédites, réorganisées par classes, échantillons et fenêtres\n",
    "    \"\"\"\n",
    "    len_classes = X.shape[0]  # Nombre de classes\n",
    "    all_features = []\n",
    "    indices = []  # Pour garder la trace des indices originaux\n",
    "\n",
    "    # 1. Extraction des features sous forme de liste\n",
    "    for class_idx in range(len_classes):\n",
    "        for sample_idx, feature_matrix in enumerate(X[class_idx]):  # X[class_idx] est de shape (variable, n_win, fv_len)\n",
    "            num_windows = feature_matrix.shape[0]  # Nombre de fenêtres pour cet échantillon\n",
    "            all_features.append(feature_matrix)  # Stocke les features\n",
    "            indices.extend([(class_idx, sample_idx)] * num_windows)  # Associe chaque fenêtre à son (class, sample)\n",
    "\n",
    "    # 2. Conversion en un array unique pour accélérer la prédiction\n",
    "    X_flattened = np.vstack(all_features)  # Shape: (total_windows, fv_len)\n",
    "    X_flattened = X_flattened / np.linalg.norm(X_flattened, axis=0)  # Normal\n",
    "    X_flattened = pca.transform(X_flattened)  # Réduit les dimensions\n",
    "\n",
    "    # 3. Prédiction des probabilités en batch\n",
    "    probas_flattened = clf.predict_proba(X_flattened)  # Sortie: (total_windows, n_classes)\n",
    "\n",
    "    # 4. Réorganisation des probabilités\n",
    "    probabilities = [[] for _ in range(len_classes)]\n",
    "    for (class_idx, sample_idx), proba in zip(indices, probas_flattened):\n",
    "        if len(probabilities[class_idx]) <= sample_idx:\n",
    "            probabilities[class_idx].append([])  # Assure que la liste existe\n",
    "        probabilities[class_idx][sample_idx].append(proba)  # Ajoute la probabilité pour chaque fenêtre\n",
    "\n",
    "    return probabilities  # Liste [classes][samples][windows][probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_naive(probs):\n",
    "    \"\"\"Règle naïve : Choisir la classe avec la probabilité la plus élevée.\"\"\"\n",
    "    return classnames[np.argmax(probs) % len(classnames)]\n",
    "\n",
    "def decision_majority(probs):\n",
    "    \"\"\"Règle du vote majoritaire : Choisir la classe avec le plus grand nombre de votes.\"\"\"\n",
    "    votes = np.argmax(probs, axis=1)\n",
    "    count = np.bincount(votes)\n",
    "    max_indices = np.where(count == count.max())[0]\n",
    "    return classnames[max_indices[0]]  # En cas d'égalité, on prend la première classe\n",
    "\n",
    "def decision_weighted(probs):\n",
    "    \"\"\"Règle du vote pondéré : Choisir la classe avec la somme maximale des probabilités.\"\"\"\n",
    "    sum_probs = np.sum(probs, axis=0)\n",
    "    return classnames[np.argmax(sum_probs)]\n",
    "\n",
    "def decision_maxlikelihood(probs):\n",
    "    \"\"\"Règle du maximum de vraisemblance : Choisir la classe avec le produit maximal des probabilités.\"\"\"\n",
    "    prod_probs = np.prod(probs, axis=0)\n",
    "    return classnames[np.argmax(prod_probs)]\n",
    "\n",
    "# Application de la règle de décision à chaque échantillon d'un audio\n",
    "def apply_decision_rules(probabilities, decision_method):\n",
    "    \"\"\"\n",
    "    Applique une règle de décision sur toutes les fenêtres d'un échantillon.\n",
    "\n",
    "    :param probabilities: Probabilités des fenêtres pour un échantillon (n_win, n_classes)\n",
    "    :param decision_method: Méthode de décision à utiliser parmi 'naive', 'majority', 'weighted', 'maxlikelihood' [0, 1, 2, 3]\n",
    "    :return: Classe prédite pour l'échantillon\n",
    "    \"\"\"\n",
    "    if decision_method == 0:\n",
    "        return decision_naive(probabilities)\n",
    "    elif decision_method == 1:\n",
    "        return decision_majority(probabilities)\n",
    "    elif decision_method == 2:\n",
    "        return decision_weighted(probabilities)\n",
    "    elif decision_method == 3:\n",
    "        return decision_maxlikelihood(probabilities)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown decision method: \" + decision_method)\n",
    "    \n",
    "dict_decision_methods = {\n",
    "    0: \"Naive\",\n",
    "    1: \"Majority\",\n",
    "    2: \"Weighted\",\n",
    "    3: \"MaxLikelihood\"\n",
    "}\n",
    "\n",
    "\n",
    "def predict_for_all_samples(probabilities, decision_method):\n",
    "    \"\"\"\n",
    "    Applique une règle de décision sur tous les échantillons (classes, samples) du dataset.\n",
    "\n",
    "    :param probabilities: Liste des probabilités pour chaque fenêtre de chaque échantillon.\n",
    "    :param decision_method: Méthode de décision ('naive', 'majority', 'weighted', 'maxlikelihood')\n",
    "    :return: Liste des classes prédites pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n_classes = len(probabilities)\n",
    "    n_samples = len(probabilities[0])\n",
    "    predictions = []\n",
    "\n",
    "    for class_idx in range(n_classes):\n",
    "        class_predictions = []\n",
    "        for sample_idx in range(n_samples):\n",
    "            # Probabilités des fenêtres pour un échantillon donné\n",
    "            probs = probabilities[class_idx][sample_idx]\n",
    "            # Applique la règle de décision sur l'échantillon\n",
    "            predicted_class = apply_decision_rules(probs, decision_method)\n",
    "            class_predictions.append(predicted_class)\n",
    "        predictions.append(class_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(predictions, y_val):\n",
    "    \"\"\"\n",
    "    Calcule l'accuracy en comparant les prédictions avec les labels réels.\n",
    "\n",
    "    :param predictions: Liste des prédictions pour chaque échantillon et chaque classe\n",
    "                        (n_classes, n_samples) - chaque élément est la classe prédite pour un échantillon.\n",
    "    :param y_val: Array des labels réels pour chaque échantillon (n_classes, n_samples)\n",
    "                  Chaque élément est la classe réelle de cet échantillon.\n",
    "    :return: L'accuracy du modèle en pourcentage\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    n_classes = len(predictions)\n",
    "\n",
    "    for class_idx in range(n_classes):\n",
    "        for sample_idx in range(len(predictions[class_idx])):\n",
    "            # Comparer la prédiction avec le label réel\n",
    "            predicted_class = predictions[class_idx][sample_idx]\n",
    "            true_class = y_val[class_idx][sample_idx][0]\n",
    "            \n",
    "            # Si la prédiction est correcte, on incrémente le compteur\n",
    "            if predicted_class == true_class:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    # Calculer l'accuracy\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Temp\\ipykernel_21236\\1821133816.py:73: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_RF = results_RF._append({\"model\": \"Random Forest\", \"n_estimators\": n, \"max_depth\": d, \"min_samples_split\": s, \"accuracy\": accuracies[decision_index], \"decision_method\": dict_decision_methods[decision_index], \"Nft\": Nft, \"nmel\": nmel, \"pca_comp\": pca_comp}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 79.208333\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            15\n",
      "pca_comp                        50\n",
      "Name: 3, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 79.208333\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            15\n",
      "pca_comp                        50\n",
      "Name: 3, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 79.208333\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            15\n",
      "pca_comp                        50\n",
      "Name: 3, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 81.740385\n",
      "decision_method      MaxLikelihood\n",
      "Nft                            256\n",
      "nmel                            20\n",
      "pca_comp                        50\n",
      "Name: 15, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 83.019231\n",
      "decision_method      MaxLikelihood\n",
      "Nft                           1024\n",
      "nmel                            15\n",
      "pca_comp                        50\n",
      "Name: 75, dtype: object\n",
      "model                Random Forest\n",
      "n_estimators                   100\n",
      "max_depth                       20\n",
      "min_samples_split                5\n",
      "accuracy                 83.019231\n",
      "decision_method      MaxLikelihood\n",
      "Nft                           1024\n",
      "nmel                            15\n",
      "pca_comp                        50\n",
      "Name: 75, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=200 must be between 0 and min(n_samples, n_features)=150 with svd_solver='covariance_eigh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pca_comp \u001b[38;5;129;01min\u001b[39;00m pca_comp_list:\n\u001b[0;32m     34\u001b[0m     fv_len, n_win_files \u001b[38;5;241m=\u001b[39m fv_generator(Nft, nmel)\n\u001b[1;32m---> 35\u001b[0m     pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_comp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_estimators:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m max_depth:\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mpca_generator\u001b[1;34m(training_indexes, pca_comp)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m     53\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mpca_comp)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pca\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pca\n",
      "File \u001b[1;32m~\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:448\u001b[0m, in \u001b[0;36mPCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:547\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32m~\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:561\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         )\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=200 must be between 0 and min(n_samples, n_features)=150 with svd_solver='covariance_eigh'"
     ]
    }
   ],
   "source": [
    "# we will try training in a different way now, basically we want that the models fits to melspectrogram of the audio files of a subset of the training set\n",
    "# but for the validation, we want to evaluate it on a sequence of melspectrogram from the complementary subset of the training set\n",
    "\n",
    "# Model training\n",
    "# we will test three models, CNN, SVM and Random Forest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TO DO : implement the CNN model\n",
    "\n",
    "# 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "kfold_indexes = [[], [], [], []]\n",
    "for class_index in range (len(classnames)):\n",
    "    kfold_indexes[class_index] = manual_kfold_random(random_indexes[class_index], n_splits, seed=seed)\n",
    "\n",
    "# hyperparameters of the spectrogram\n",
    "Nft_list = [256, 512, 1024]\n",
    "nmel_list = [15, 20, 30]\n",
    "pca_comp_list = [50, 100]\n",
    "\n",
    "# Random Forest\n",
    "n_estimators = [100]\n",
    "max_depth = [20]\n",
    "min_samples_split = [5]\n",
    "\n",
    "results_RF = pd.DataFrame(columns=[\"model\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"accuracy\", \"decision_method\", \"Nft\", \"nmel\", \"pca_comp\"])\n",
    "\n",
    "for Nft in Nft_list:\n",
    "    for nmel in nmel_list:\n",
    "        for pca_comp in pca_comp_list:\n",
    "            fv_len, n_win_files = fv_generator(Nft, nmel)\n",
    "            pca = pca_generator(random_indexes, pca_comp)\n",
    "            for n in n_estimators:\n",
    "                for d in max_depth:\n",
    "                    for s in min_samples_split:\n",
    "                        clf = RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_split=s)\n",
    "                        accuracies = np.zeros(len(classnames))\n",
    "                        \n",
    "                        # creating the indexes for each split\n",
    "                        kfold_indexes = [[], [], [], []]\n",
    "                        for class_index in range (len(classnames)):\n",
    "                            kfold_indexes[class_index] = manual_kfold_random(random_indexes[class_index], n_splits, seed=seed)\n",
    "                            \n",
    "                        # kfold cross validation\n",
    "                        for split in range(n_splits):\n",
    "                            X_train = np.empty(len(classnames), dtype=object)\n",
    "                            y_train = np.empty(len(classnames), dtype=object)\n",
    "                            X_val = np.empty(len(classnames), dtype=object)\n",
    "                            y_val = np.empty(len(classnames), dtype=object)\n",
    "                            for class_index in range (len(classnames)):\n",
    "                                X_train[class_index], y_train[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][0])\n",
    "                                X_val[class_index], y_val[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][1])\n",
    "                                \n",
    "                            # Training\n",
    "                            X_train = np.vstack([sample for class_samples in X_train for sample in class_samples])\n",
    "                            y_train = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_train, y_train) for label in labels])\n",
    "                            y_train = y_train[::fv_len]\n",
    "                            X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "                            X_train = pca.transform(X_train)\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            \n",
    "                            # Validation\n",
    "                            probas = predict_all_probabilities(clf, X_val)\n",
    "                            for decision_index in range(4):\n",
    "                                accuracies[decision_index] += compute_accuracy(predict_for_all_samples(probas,decision_index), y_val)\n",
    "                                \n",
    "                        # averaging the accuracies\n",
    "                        accuracies /= n_splits\n",
    "                        for decision_index in range(4):\n",
    "                            results_RF = results_RF._append({\"model\": \"Random Forest\", \"n_estimators\": n, \"max_depth\": d, \"min_samples_split\": s, \"accuracy\": accuracies[decision_index], \"decision_method\": dict_decision_methods[decision_index], \"Nft\": Nft, \"nmel\": nmel, \"pca_comp\": pca_comp}, ignore_index=True)\n",
    "                    print(results_RF.loc[results_RF[\"accuracy\"].idxmax()])\n",
    "\n",
    "# Save the results in .csv\n",
    "results_RF.to_csv(\"results_RF.csv\")\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD Model training\n",
    "# we will test three models, CNN, SVM and Random Forest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TO DO : implement the CNN model\n",
    "\n",
    "# 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Random Forest\n",
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [5, 10, 20, 50, 100]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "\n",
    "results_RF = pd.DataFrame(columns=[\"model\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"accuracy\"])\n",
    "\n",
    "for n in n_estimators:\n",
    "    for d in max_depth:\n",
    "        for s in min_samples_split:\n",
    "            clf = RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_split=s)\n",
    "            acc = 0\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                X_train_kf, X_test_kf = X_train[train_index], X_train[test_index]\n",
    "                y_train_kf, y_test_kf = y_train[train_index], y_train[test_index]\n",
    "                clf.fit(X_train_kf, y_train_kf)\n",
    "                acc += accuracy(y_test_kf, clf.predict(X_test_kf))\n",
    "            acc /= 5\n",
    "            results_RF = results_RF._append({\"model\": \"Random Forest\", \"n_estimators\": n, \"max_depth\": d, \"min_samples_split\": s, \"accuracy\": acc}, ignore_index=True)\n",
    "        print(results_RF.loc[results_RF[\"accuracy\"].idxmax()])\n",
    "        \n",
    "# save the results in a .csv file\n",
    "results_RF.to_csv(\"results_RF.csv\")\n",
    "\n",
    "\n",
    "# SVM\n",
    "kernel = [\"rbf\"]\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gamma = [\"scale\", \"auto\", 0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "# the results will be saved in a pandas dataframe\n",
    "results_SVM = pd.DataFrame(columns=[\"model\", \"kernel\", \"C\", \"gamma\", \"accuracy\"])\n",
    "\n",
    "for k in kernel:\n",
    "    for c in C:\n",
    "        for g in gamma:\n",
    "            clf = SVC(kernel=k, C=c, gamma=g)\n",
    "            acc = 0\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                X_train_kf, X_test_kf = X_train[train_index], X_train[test_index]\n",
    "                y_train_kf, y_test_kf = y_train[train_index], y_train[test_index]\n",
    "                clf.fit(X_train_kf, y_train_kf)\n",
    "                acc += accuracy(y_test_kf, clf.predict(X_test_kf))\n",
    "            acc /= 5\n",
    "            results_SVM = results_SVM._append({\"model\": \"SVM\", \"kernel\": k, \"C\": c, \"gamma\": g, \"accuracy\": acc}, ignore_index=True)\n",
    "        print(results_SVM.loc[results_SVM[\"accuracy\"].idxmax()])\n",
    "\n",
    "# save the results in a .csv file\n",
    "results_SVM.to_csv(\"results_SVM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
